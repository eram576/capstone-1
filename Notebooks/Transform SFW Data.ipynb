{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import texthero as hero\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "filename = 'replace_spelling.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dict(filename):\n",
    "    mydict = np.load(filename, allow_pickle=True) \n",
    "    spelling_keys = mydict.item().keys()\n",
    "    spelling_values = mydict.item().values()\n",
    "    spelling_dict = dict(zip(spelling_keys, spelling_values))\n",
    "    \n",
    "    return spelling_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all(text, mydict):\n",
    "    for gb, us in mydict.items():\n",
    "        text = text.replace(us, gb)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    STOPWORDS = set(stopwords)\n",
    "\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_col(sfw, spelling_dict):\n",
    "    \n",
    "    sfw['prof + knowlg + ability'] = \"\"\n",
    "    for i in range(0, len(sfw)):\n",
    "        sfw.loc[i, 'prof + knowlg + ability'] = str(sfw.loc[i, 'Proficiency']) + str(sfw.loc[i, 'Knowledge']) + str(sfw.loc[i, 'Abilities'])\n",
    "    \n",
    "    sfw['clean_docs'] = sfw['prof + knowlg + ability'].apply(lambda x: x.replace('\\n', '. '))\n",
    "    sfw['clean_docs'] = sfw['clean_docs'].apply(lambda x: x.replace('•', ' '))\n",
    "    sfw['clean_docs'] = sfw['prof + knowlg + ability'].apply(lambda x: x.replace('\\n', '. '))\n",
    "    sfw['clean_docs'] = sfw['clean_docs'].apply(lambda x: x.replace('•', ' '))   \n",
    "    sfw['clean_docs'] = sfw['clean_docs'].apply(lambda x: replace_all(x, spelling_dict))\n",
    "    sfw['clean_docs'] = sfw['clean_docs'].astype(str).apply(lambda x: clean_text(x, STOPWORDS))\n",
    "    sfw['Proficiency Level'] = sfw['Proficiency Level'].str.split(n=1).str[1]\n",
    "    sfw = sfw[[\"Job Role\",\"clean_docs\", \"Sector\", \"Skill Title\", \"Proficiency Level\"]]\n",
    "    \n",
    "    return sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_values(df):\n",
    "    jd_list = []\n",
    "    counter = 0\n",
    "    for row in df[1]:\n",
    "        jd=\"\"\n",
    "        counter+=1\n",
    "        for val in row:\n",
    "            jd+=val[0]\n",
    "        jd_list.append(jd)\n",
    "    return jd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skill_description(sfw):\n",
    "    sfw_concatenated = sfw.groupby(['Sector','Skill Title','Proficiency Level'])['clean_docs'].unique()\n",
    "    sfw_concatenated = pd.DataFrame(sfw_concatenated)\n",
    "    sfw_concatenated  = sfw_concatenated.groupby(['Sector','Skill Title'])['clean_docs']\n",
    "    sfw_concatenated = pd.DataFrame(sfw_concatenated)\n",
    "    \n",
    "    jd_list = concat_values(sfw_concatenated)\n",
    "    \n",
    "    sfw_concatenated['Skill description'] = jd_list\n",
    "    sfw_concatenated[['Sector', 'Skill Title']] = pd.DataFrame(sfw_concatenated[0].tolist()) \n",
    "    sfw_concatenated = sfw_concatenated.drop(columns = [0, 1])\n",
    "    sfw_concatenated = sfw_concatenated[['Sector', 'Skill Title', 'Skill description']]\n",
    "    \n",
    "    sfw_new = sfw.merge(sfw_concatenated, on=[\"Sector\",\"Skill Title\"], how = 'left')\n",
    "\n",
    "    sf_model_final = sfw_new[['Job Role', 'Sector', 'Skill Title', 'Skill description']]\n",
    "    \n",
    "    sf_model_final['Skill description'] = hero.remove_stopwords(sf_model_final['Skill description'].astype(str))\n",
    "    sf_model_final['Skill description'] = hero.remove_urls(sf_model_final['Skill description'])\n",
    "    sf_model_final['Skill description'] = hero.tokenize(sf_model_final['Skill description']).astype(str)\n",
    "    sf_model_final['Skill description'] = hero.clean(sf_model_final['Skill description'])\n",
    "\n",
    "    return sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_hierarchy(title):\n",
    "    \n",
    "    patterns = [\"Senior\", \"Junior\", \"Manager\", \"Executive\", \"Director\", \"Lead\", \"Head of\", \"(Specialist)\", \"Specialist\"]\n",
    "    \n",
    "    for s in patterns:\n",
    "        title = re.sub(s, '', title)\n",
    "        \n",
    "    split = title.split()\n",
    "    if (len(split)!=0):\n",
    "        \n",
    "        if (split[0] == 'Assistant') or (split[0]=='Head'):\n",
    "            title = \" \".join(split[1:])\n",
    "\n",
    "        if title[-2:]==\"or\":\n",
    "            title = title[:-2].strip()+\"ing\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    list_ = []\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        list_.append(lemmatize_stemming(w))\n",
    "    \n",
    "    title = ' '.join(list_)\n",
    "  \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_Jobs(sfw):\n",
    "    sfw['Job_Role_Normal'] = sfw['Job Role'].apply(clear_hierarchy)\n",
    "    sfw['Job_Role_Stemmed'] = sfw.Job_Role_Normal.apply(lemmatize_text)\n",
    "    sfw = sfw[sfw['Job_Role_Normal']!='']\n",
    "    \n",
    "    return sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(x):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        y.append(x[i].lower())\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlist_SkillTitles(sfw):\n",
    "    sfw['Skill Title'] = sfw['Skill Title'].apply(to_lower)\n",
    "    sfw['Skill Title'] = sfw['Skill Title'].astype(str)\n",
    "    sfw['Skill Title'] = sfw['Skill Title'].apply(lambda x: x.replace(\"'\", \"\"))\n",
    "    sfw['Skills_unlisted'] = sfw['Skill Title'].apply(lambda x: x.replace('[','').replace(']','').replace(\",\", \" \"))\n",
    "    \n",
    "    return sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_skills(sfw):\n",
    "    sfw = sfw.groupby(['Job_Role_Stemmed', 'Sector']).agg({'Job Role': lambda series: list(series), 'Skill description': lambda series: list(series), 'Skill Title':lambda series: list(series)})\n",
    "    sfw.reset_index(inplace=True)\n",
    "    sfw['Job Role'] = sfw['Job Role'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    \n",
    "    return sfw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    \n",
    "    data = new_col(data, spelling_dict)\n",
    "    data = create_skill_description(data)\n",
    "    data = normalise_Jobs(data)\n",
    "    data = group_skills(data)\n",
    "    data = unlist_SkillTitles(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFW_16  = pd.read_excel('Enhanced construct - 16 sectors.xlsx - Job Role_TSC.xlsx')\n",
    "\n",
    "SFW_13  = pd.read_excel('Enhanced construct - 13 sectors.xlsx - Job Role_TSC.xlsx')\n",
    "\n",
    "sfw = pd.concat([SFW_16, SFW_13], sort = False, ignore_index = True)\n",
    "\n",
    "sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_dict = retrieve_dict(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = transform_data(sfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sf_model_final.to_csv('sf_model_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
