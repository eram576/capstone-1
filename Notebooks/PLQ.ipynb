{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import smart_open\n",
    "\n",
    "import logging\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "import docx2txt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim==3.4.0\n",
    "# !pip install smart_open==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/valli/Google Drive/Dell/Semester 6/Capstone/Data/SFW_13_cleaned_summarised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"C:/Users/valli/Google Drive/Dell/Semester 6/Capstone/Data/SFW_16_cleaned_summarised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.concat([df, df2], sort = False, ignore_index = True).drop(columns = ['Unnamed: 0'])\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf[(sf['Skill Title']==\"Accident and Incident Response Management\")&(sf['Sector']==\"Air Transport\")].sort_values('Proficiency Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf[(sf['Skill Title']==\"Accident and Incident Response Management\")&(sf['Sector']==\"Tourism\")].sort_values('Proficiency Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_summarised = sf[['Sector', 'Skill Title', 'Proficiency Level', 'summary_jd']].groupby(['Sector', 'Skill Title', 'Proficiency Level']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_summarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = sf['Sector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_new = sf[~sf['Sector'].isin(sectors[sectors == 1].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model = sf_new[[\"Job Role\",\"clean_docs\", \"summary_jd\", \"Sector\", \"Skill Title\", \"Proficiency Level\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Portal import *\n",
    "# pd.set_option('display.width', 400)\n",
    "# pd.set_option('display.max_columns', 10)\n",
    "\n",
    "\n",
    "class CareersFuture(Portal):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def read_data(self):\n",
    "        # Reads data\n",
    "        df = pd.read_csv('mycareerfuture2020-09-15.csv', error_bad_lines=False)\n",
    "        return df\n",
    "\n",
    "    def clean_data(self):\n",
    "        df = self.read_data()\n",
    "        # df = df[df['Job Description'].str.contains('value')]\n",
    "        df['Job Description'] = df['Job Description'].astype(str)\n",
    "        df['Job Description'] = df['Job Description'].apply(lambda x: self.strip_html_tags(x))\n",
    "\n",
    "        df['Industry'] = df['Industry'].astype(str)\n",
    "        df['Industry'] = df['Industry'].apply(lambda x: self.str_to_literal(x))\n",
    "\n",
    "        # df['Skills'] = df['Skills'].astype(str)\n",
    "\n",
    "        df['Job Experience Required (years)'] = df['Job Experience Required (years)'].astype(int)\n",
    "\n",
    "        df['Job Monthly Min Sal'] = df['Job Monthly Min Sal'].astype(int)\n",
    "        df['Job Monthly Max Sal'] = df['Job Monthly Max Sal'].astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def str_to_literal(self, text):\n",
    "        try:\n",
    "            # ast.literal_eval function converts takes in a string and converts it into a dictionary\n",
    "            ls = super().str_to_literal(text)\n",
    "            industry = []\n",
    "            for i in ls:\n",
    "                industry.append(i['category'])\n",
    "            return industry\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "careersfuture = CareersFuture(\"careersfuture\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = pd.DataFrame(careersfuture.clean_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['Industry_unlisted'] = cf['Industry'].apply(lambda x: ','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['Industry_unlisted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['Skills'] = cf['Skills'].apply(lambda x: x.replace(\"'\", \"\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['Skills_unlisted'] = cf['Skills'].apply(lambda x: x.replace('[','').replace(']',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model = cf[[\"Job Title\", \"Job Description\", \"Industry_unlisted\", \"Skills_unlisted\", \"Job Experience Required (years)\",\"Job Monthly Min Sal\", \"Job Monthly Max Sal\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = pd.DataFrame(cf_model.Industry_unlisted.value_counts().nlargest(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "\n",
    "# print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model['clean_docs'] = sf_model['clean_docs'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model['summary_jd'] = sf_model['summary_jd'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model['Proficiency Level'] = sf_model['Proficiency Level'].str.split(n=1).str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_skills_concatenated = sf_model.groupby(['Sector','Skill Title','Proficiency Level'])['clean_docs'].unique()\n",
    "sf_skills_concatenated = pd.DataFrame(sf_skills_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_skills_concatenated  = sf_skills_concatenated.groupby(['Sector','Skill Title'])['clean_docs']\n",
    "sf_skills_concatenated = pd.DataFrame(sf_skills_concatenated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_values(df):\n",
    "    jd_list = []\n",
    "    counter = 0\n",
    "    for row in df[1]:\n",
    "        jd=\"\"\n",
    "        counter+=1\n",
    "        for val in row:\n",
    "            jd+=val[0]\n",
    "        jd_list.append(jd)\n",
    "    return jd_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_list = concat_values(sf_skills_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_skills_concatenated['Skill description'] = jd_list\n",
    "sf_skills_concatenated[['Sector', 'Skill Title']] = pd.DataFrame(sf_skills_concatenated[0].tolist()) \n",
    "sf_skills_concatenated = sf_skills_concatenated.drop(columns = [0, 1])\n",
    "sf_skills_concatenated = sf_skills_concatenated[['Sector', 'Skill Title', 'Skill description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_new = sf_model.merge(sf_skills_concatenated, on=[\"Sector\",\"Skill Title\"], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = sf_model_new[['Job Role', 'Sector', 'Skill Title', 'Skill description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final.to_csv(index = False, path_or_buf=\"sf_skills_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = pd.read_csv(\"sf_skills_concat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Skill description'] = hero.remove_stopwords(sf_model_final['Skill description'].astype(str))\n",
    "sf_model_final['Skill description'] = hero.remove_urls(sf_model_final['Skill description'])\n",
    "sf_model_final['Skill description'] = hero.tokenize(sf_model_final['Skill description']).astype(str)\n",
    "sf_model_final['Skill description'] = hero.clean(sf_model_final['Skill description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model['Job Description'] = hero.remove_stopwords(cf_model['Job Description'].astype(str))\n",
    "cf_model['Job Description'] = hero.remove_urls(cf_model['Job Description'])\n",
    "cf_model['Job Description'] = hero.tokenize(cf_model['Job Description']).astype(str)\n",
    "cf_model['Job Description'] = hero.clean(cf_model['Job Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "jobD = [row.split() for row in sf_model_final.loc[:, 'Skill description']]\n",
    "phrases = Phrases(jobD, min_count=10, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[jobD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "cf_jobD = [row.split() for row in sf_model_final.loc[:, 'Skill description']]\n",
    "cf_phrases = Phrases(cf_jobD, min_count=10, progress_per=10000)\n",
    "cf_bigram = Phraser(cf_phrases)\n",
    "cf_sentences = cf_bigram[cf_jobD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Skill Description'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Skill Description'] = sf_model_final['Skill Description'].apply(lambda x: ','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job Role']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = sf_model_final.reset_index().drop(columns = ['index'])\n",
    "sf_model_final = sf_model_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Senior', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Junior', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Manager', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Executive', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Director', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Lead', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Head of ', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('(Specialist)', '')\n",
    "sf_model_final['Job_Role_Replaced'] = sf_model_final['Job_Role_Replaced'].str.replace('Specialist', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_hierarchy(title):\n",
    "    \n",
    "    patterns = [\"Senior\", \"Junior\", \"Manager\", \"Executive\", \"Director\", \"Lead\", \"Head of\", \"(Specialist)\", \"Specialist\"]\n",
    "    for s in patterns:\n",
    "        title = re.sub(s, '',title)\n",
    "        \n",
    "    split = title.split()\n",
    "    if (split[0] == 'Assistant') or (split[0]=='Head'):\n",
    "        title = \" \".join(split[1:])\n",
    "\n",
    "    if title[-2:]==\"or\":\n",
    "        title = title[:-2].strip()+\"ing\"\n",
    "   \n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    list_ = []\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        list_.append(lemmatize_stemming(w))\n",
    "    \n",
    "    title = ' '.join(list_)\n",
    "  \n",
    "    return list_\n",
    "\n",
    "\n",
    "\n",
    "sf_model_final['Job_Role_Normal'] = sf_model_final['Job Role'].apply(lambda x: clear_hierarchy(x))\n",
    "sf_model_final['Job_Role_Stemmed'] = sf_model_final.Job_Role_Normal.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final[sf_model_final['Sector']==\"Infocomm Technology\"]['Job_Role_Replaced'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final.to_csv(\"sf_model_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final[(sf_model_final['Job Role']=='Marketing Manager') & (sf_model_final['Sector']=='Infocomm Technology')]['Skill Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = pd.read_csv(\"sf_model_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT = sf_model_final[sf_model_final['Sector']==\"Infocomm Technology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model_IT = cf_model[cf_model['Industry_unlisted']==\"Information Technology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sf_model_IT['Skill Description']\n",
    "y_train = sf_model_IT['Job_Role_Replaced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cf_model_IT['Job Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sf_model_IT[sf_model_IT['Job Role'] == 'Associate Systems Support Engineer'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(idx)\n",
    "y_train = y_train.drop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_sf, X_test_sf, y_train_sf, y_test_sf = train_test_split(X_train, \n",
    "                                                    y_train, \n",
    "                                                    test_size = .3, \n",
    "                                                    shuffle = True, \n",
    "                                                    stratify = y_train, \n",
    "                                                    random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = pd.read_csv('CareersFutureCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sf = pd.read_csv('sf_model_final_final.csv')\n",
    "sf = pd.read_csv('sf_model_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf['Skill Title'] = sf['Skill Title'].apply(lambda x: x.replace(\"['\", \"\").replace(\"']\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                \n",
    "               ])\n",
    "logreg.fit(sf['Skill Title'], sf['Job Role'])\n",
    "\n",
    "\n",
    "\n",
    "#y_pred = logreg.predict()\n",
    "\n",
    "\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred,target_names=ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = sf['Job Role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Str_Sort_Match = fuzz.partial_ratio('Science data', 'Data Scientist and DevSecOps Developer')\n",
    "print(\"String Matched:\",Str_Sort_Match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similary_jobs(title):\n",
    "    max = 50\n",
    "    return_value = \"\"\n",
    "    for role in roles:\n",
    "        if fuzz.partial_ratio(title, role)>max:\n",
    "            max = fuzz.partial_ratio(title, role)\n",
    "            return_value = role\n",
    "    return return_value\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_subset = cf[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_subset['SFW_Job'] = cf_subset['Job Title'].apply(lambda x:check_similary_jobs(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = cf_subset[cf_subset['SFW_Job']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Skills'] = test_data['Skills'].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(test_data['Skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['SFW_Job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_data['SFW_Job'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.read_csv('sf_model_final_final.csv')\n",
    "sf['Skill Title'] = sf['Skill Title'].apply(lambda x: x.replace(\"['\", \"\").replace(\"']\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                \n",
    "               ])\n",
    "logreg.fit(sf['Skill Title'], sf['Job Role'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(test_data['Skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['SFW_Job'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(df, pred):\n",
    "    counter = 0\n",
    "    for i in range(len(df)):\n",
    "        if df['SFW_Job'][i] in pred[i]:\n",
    "            counter+=1\n",
    "            print(df['SFW_Job'][i])\n",
    "            print(pred[i])\n",
    "    \n",
    "    return counter/len(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(test_data, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_sf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(X_test).reset_index()\n",
    "Y_test = pd.DataFrame(cf_model_IT['Job Title']).reset_index()\n",
    "Y_pred = pd.DataFrame(y_pred).reset_index()\n",
    "results = pd.concat([results, Y_test, Y_pred[0]], axis = 1, ignore_index=True)\n",
    "results = results.drop(columns = [0,2])\n",
    "results.columns = [\"CF Decription\", \"CF Job Title\", \"SF Job Title Matching\"]\n",
    "results.to_csv(\"log_results_no_bigram.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "# wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', DecisionTreeClassifier(random_state=3)),\n",
    "                \n",
    "               ])\n",
    "DT.fit(X_train_sf, y_train_sf)\n",
    "\n",
    "\n",
    "y_pred = DT.predict(X_test_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_sf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = sf_model_IT['Skill description']\n",
    "\n",
    "KN = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "                \n",
    "               ])\n",
    "KN.fit(X_train_sf, y_train_sf)\n",
    "\n",
    "\n",
    "y_pred = KN.predict(X_test_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_sf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(X_test).reset_index()\n",
    "Y_test = pd.DataFrame(cf_model_IT['Job Title']).reset_index()\n",
    "Y_pred = pd.DataFrame(y_pred).reset_index()\n",
    "results = pd.concat([results, Y_test, Y_pred[0]], axis = 1, ignore_index=True)\n",
    "results = results.drop(columns = [0,2])\n",
    "results.columns = [\"CF Decription\", \"CF Job Title\", \"SF Job Title Matching\"]\n",
    "results.to_csv(\"KN_no_bigrams.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RF = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators = 50)),\n",
    "                \n",
    "               ])\n",
    "RF.fit(X_train_sf, y_train_sf)\n",
    "\n",
    "\n",
    "y_pred = RF.predict(X_test_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_sf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(alpha=1e-06,\n",
    "               loss='log',\n",
    "               max_iter=1000,\n",
    "               penalty='l2',\n",
    "               learning_rate = 'constant',\n",
    "               eta0 = .1,\n",
    "               random_state = 3,\n",
    "               tol=None)),\n",
    "                \n",
    "               ])\n",
    "SGD.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = SGD.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(X_test).reset_index()\n",
    "Y_test = pd.DataFrame(cf_model_IT['Job Title']).reset_index()\n",
    "Y_pred = pd.DataFrame(y_pred).reset_index()\n",
    "results = pd.concat([results, Y_test, Y_pred[0]], axis = 1, ignore_index=True)\n",
    "results = results.drop(columns = [0,2])\n",
    "results.columns = [\"CF Decription\", \"CF Job Title\", \"SF Job Title Matching\"]\n",
    "results.to_csv(\"SGD_no_bigrams.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(alpha=1e-06,\n",
    "               loss='log',\n",
    "               max_iter=1000,\n",
    "               penalty='l2',\n",
    "               learning_rate = 'constant',\n",
    "               eta0 = .1,\n",
    "               random_state = 3,\n",
    "               tol=None)),\n",
    "                \n",
    "               ])\n",
    "SGD.fit(X_train_sf, y_train_sf)\n",
    "\n",
    "\n",
    "y_pred_sf = SGD.predict(X_test_sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_sf, y_pred_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_sf, y_pred_sf) \n",
    "\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = y_train_sf.unique(), \n",
    "                     columns = y_train_sf.unique())\n",
    "\n",
    "#Plot the heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "sns.heatmap(cm_df, \n",
    "            center=0, \n",
    "            cmap=sns.diverging_palette(220, 15, as_cmap=True), \n",
    "            annot=True, \n",
    "            fmt='g')\n",
    "\n",
    "plt.title('SGD (loss = log) \\nF1 Score (avg = macro) : {0:.2f}'.format(f1_score(y_test_sf, y_pred_sf, average='macro')), fontsize = 13)\n",
    "plt.ylabel('True label', fontsize = 13)\n",
    "plt.xlabel('Predicted label', fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf_model_IT[sf_model_IT['Skill Title']==\"Data Design\"]\n",
    "sf_model_IT[sf_model_IT['Job Role']==\"Data Engineer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_jobs_concatenated = sf_model_IT.groupby(['Job Role','Skill Title'])['Skill description'].unique()\n",
    "sf_jobs_concatenated = pd.DataFrame(sf_jobs_concatenated)\n",
    "\n",
    "sf_jobs_concatenated = sf_jobs_concatenated.reset_index(level=[0,1])\n",
    "\n",
    "sf_jobs_concatenated  = sf_jobs_concatenated[[\"Job Role\", \"Skill description\"]].groupby(['Job Role'])\n",
    "sf_jobs_concatenated  = pd.DataFrame(sf_jobs_concatenated  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_jobs_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_skills(df):\n",
    "    jd_list = []\n",
    "\n",
    "    for row in range(len(df)):\n",
    "        jd=\"\"\n",
    "        for val in df.iloc[row][1]['Skill description']:\n",
    "            jd+=val\n",
    "            \n",
    "        jd_list.append(jd)\n",
    "    return jd_list\n",
    "\n",
    "jd_list = concat_skills(sf_jobs_concatenated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_jobs_concatenated['Job description'] = jd_list\n",
    "sf_jobs_concatenated = sf_jobs_concatenated.drop(columns = [1])\n",
    "sf_jobs_concatenated.columns = ['Job_Role', 'Job description']\n",
    "sf_jobs_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_jobs_concatenated['Job description'] = [','.join(map(str, l)) for l in sf_jobs_concatenated['Job description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_jobs_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                \n",
    "               ])\n",
    "logreg.fit(sf_jobs_concatenated['Job description'], sf_jobs_concatenated['Job_Role'])\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [text_resume, text]\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_skills = pd.read_csv(\"sf_skills_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jobD = [row.split() for row in sf_skills['Skill description'].astype(str)]\n",
    "phrases = Phrases(jobD, min_count=10, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[jobD]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=10,\n",
    "                     window=3,\n",
    "                     size=55,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.05, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=40, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"health\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"finance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['bio', 'medicine', 'computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = []\n",
    "vecs = []\n",
    "for word in w2v_model.wv.vocab:\n",
    "    vocabs.append(word)\n",
    "    vecs.append(w2v_model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vecs = vecs[:100]\n",
    "sub_vocab = vocabs[:100]\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2000, random_state=23)\n",
    "new_values = tsne_model.fit_transform(sub_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(sub_vocab[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT.columns = [c.replace(' ', '_') for c in sf_model_IT.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT = sf_model_IT.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = sf_model_IT.Job_Role.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT['label'] = sf_model_IT_BERT.Job_Role.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT = sf_model_IT_BERT.drop(sf_model_IT_BERT[sf_model_IT_BERT['label'] == 16].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT.to_csv(\"sf_model_IT_Bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(sf_model_IT_BERT.index.values, \n",
    "                                                  sf_model_IT_BERT.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=sf_model_IT_BERT.label.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT['data_type'] = ['not_set']*sf_model_IT_BERT.shape[0]\n",
    "\n",
    "sf_model_IT_BERT.loc[X_train, 'data_type'] = 'train'\n",
    "sf_model_IT_BERT.loc[X_val, 'data_type'] = 'val'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_IT_BERT[['Job_Role', 'Skill_description', 'data_type', 'label']].groupby(['Job_Role', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "#\n",
    "#from torch.utils.data import TensorDataset\n",
    "\n",
    "# from transformers import BertForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    sf_model_IT_BERT[sf_model_IT_BERT.data_type=='train'].Skill_description.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    sf_model_IT_BERT[sf_model_IT_BERT.data_type=='val'].Skill_description.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(sf_model_IT_BERT[sf_model_IT_BERT.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(sf_model_IT_BERT[sf_model_IT_BERT.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "import random\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('data_volume/finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = hero.visualization.top_words(sf_model_final.loc[:, 'Skill description']).head(20)\n",
    "top_20.plot.bar(rot=90, title=\"Top 20 words in corpus\");\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Skill Title'].value_counts().sort_values(ascending=False).head(20).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero.wordcloud(sf_model_final.loc[:, 'Skill description'], max_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=10,\n",
    "                     window=3,\n",
    "                     size=55,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.05, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "vocabs = []\n",
    "vecs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in w2v_model.wv.vocab:\n",
    "    vocabs.append(word)\n",
    "    vecs.append(w2v_model[word])\n",
    "sub_vecs = vecs[:100]\n",
    "sub_vocab = vocabs[:100]\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2000, random_state=23)\n",
    "new_values = tsne_model.fit_transform(sub_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(sub_vocab[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model_cf = Word2Vec(min_count=10,\n",
    "                     window=3,\n",
    "                     size=55,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.05, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_cf.build_vocab(cf_sentences, progress_per=10000)\n",
    "w2v_model_cf.train(cf_sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "vocabs = []\n",
    "vecs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in w2v_model_cf.wv.vocab:\n",
    "    vocabs.append(word)\n",
    "    vecs.append(w2v_model_cf[word])\n",
    "sub_vecs = vecs[:100]\n",
    "sub_vocab = vocabs[:100]\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2000, random_state=23)\n",
    "new_values = tsne_model.fit_transform(sub_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(sub_vocab[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnecessary stuff lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "print (spell('hillo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spell('organization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker(distance=1)\n",
    "test = ['na', 'lief']\n",
    "for word in test:\n",
    "  # Get the one `most likely` answer\n",
    "  print(spell.correction(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "import re, string, json\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install symspellpy\n",
    "from symspellpy import SymSpell\n",
    "from itertools import islice\n",
    "import pkg_resources\n",
    "def spell_correction(sentence_list):\n",
    "    max_edit_distance_dictionary= 3\n",
    "    prefix_length = 4\n",
    "    spellchecker = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    spellchecker.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    spellchecker.load_bigram_dictionary(dictionary_path, term_index=0, count_index=2)\n",
    "    norm_sents = []\n",
    "    print(\"Spell correcting\")\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        norm_sents.append(_spell_correction_text(sentence, spellchecker))\n",
    "    return norm_sents\n",
    "\n",
    "def _spell_correction_text(text, spellchecker):\n",
    "    \"\"\"\n",
    "    This function does very simple spell correction normalization using pyspellchecker module. It works over a tokenized sentence and only the token representations are changed.\n",
    "    \"\"\"\n",
    "    if len(text) < 1:\n",
    "        return \"\"\n",
    "    #Spell checker config\n",
    "    max_edit_distance_lookup = 2\n",
    "    suggestion_verbosity = Verbosity.TOP # TOP, CLOSEST, ALL\n",
    "    #End of Spell checker config\n",
    "    token_list = text.split()\n",
    "    for word_pos in range(len(token_list)):\n",
    "        word = token_list[word_pos]\n",
    "        if word is None:\n",
    "            token_list[word_pos] = \"\"\n",
    "            continue\n",
    "        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word.lower() in spellchecker.words.keys()):\n",
    "            suggestions = spellchecker.lookup(word.lower(), suggestion_verbosity, max_edit_distance_lookup)\n",
    "            #Checks first uppercase to conserve the case.\n",
    "            upperfirst = word[0].isupper()\n",
    "            \n",
    "            if len(suggestions) > 0:\n",
    "                correction = suggestions[0].term\n",
    "                replacement = correction\n",
    "            #We call our _reduce_exaggerations function if no suggestion is found. Maybe there are repeated chars.\n",
    "            else:\n",
    "                replacement = _reduce_exaggerations(word)\n",
    "\n",
    "            #Takes the case back to the word.\n",
    "            if upperfirst:\n",
    "                replacement = replacement[0].upper()+replacement[1:]\n",
    "            word = replacement\n",
    "            token_list[word_pos] = word\n",
    "    return \" \".join(token_list).strip()\n",
    "\n",
    "def _reduce_exaggerations(text):\n",
    "    \"\"\"\n",
    "    Auxiliary function to help with exxagerated words.\n",
    "    Examples:\n",
    "        woooooords -> words\n",
    "        yaaaaaaaaaaaaaaay -> yay\n",
    "    \"\"\"\n",
    "    correction = str(text)\n",
    "    #TODO work on complexity reduction.\n",
    "    return re.sub(r'([\\w])\\1+', r'\\1', correction)\n",
    "\n",
    "def is_numeric(text):\n",
    "    for char in text:\n",
    "        if not (char in \"0123456789\" or char in \",%.$\"):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_correction(\"Hi, hellllo. How are you doin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# # Preliminaries\n",
    "\n",
    "# from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# # Models\n",
    "\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# # Training\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Evaluation\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Model parameter\n",
    "# MAX_SEQ_LEN = 128\n",
    "# PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "# UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "\n",
    "\n",
    "# # Fields\n",
    "\n",
    "# label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "# text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "#                    fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "# fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
    "\n",
    "# # TabularDataset\n",
    "\n",
    "# train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
    "#                                            test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "# # Iterators\n",
    "\n",
    "# train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "#                             device=device, train=True, sort=True, sort_within_batch=True)\n",
    "# valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "#                             device=device, train=True, sort=True, sort_within_batch=True)\n",
    "# test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n",
    "\n",
    "# class BERT(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(BERT, self).__init__()\n",
    "\n",
    "#         options_name = \"bert-base-uncased\"\n",
    "#         self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "#     def forward(self, text, label):\n",
    "#         loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "#         return loss, text_fea\n",
    "\n",
    "# def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "#     if save_path == None:\n",
    "#         return\n",
    "    \n",
    "#     state_dict = {'model_state_dict': model.state_dict(),\n",
    "#                   'valid_loss': valid_loss}\n",
    "    \n",
    "#     torch.save(state_dict, save_path)\n",
    "#     print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "# def load_checkpoint(load_path, model):\n",
    "    \n",
    "#     if load_path==None:\n",
    "#         return\n",
    "    \n",
    "#     state_dict = torch.load(load_path, map_location=device)\n",
    "#     print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "#     model.load_state_dict(state_dict['model_state_dict'])\n",
    "#     return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "# def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "#     if save_path == None:\n",
    "#         return\n",
    "    \n",
    "#     state_dict = {'train_loss_list': train_loss_list,\n",
    "#                   'valid_loss_list': valid_loss_list,\n",
    "#                   'global_steps_list': global_steps_list}\n",
    "    \n",
    "#     torch.save(state_dict, save_path)\n",
    "#     print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "# def load_metrics(load_path):\n",
    "\n",
    "#     if load_path==None:\n",
    "#         return\n",
    "    \n",
    "#     state_dict = torch.load(load_path, map_location=device)\n",
    "#     print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "#     return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
    "\n",
    "# def train(model,\n",
    "#           optimizer,\n",
    "#           criterion = nn.BCELoss(),\n",
    "#           train_loader = train_iter,\n",
    "#           valid_loader = valid_iter,\n",
    "#           num_epochs = 5,\n",
    "#           eval_every = len(train_iter) // 2,\n",
    "#           file_path = destination_folder,\n",
    "#           best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "#     # initialize running values\n",
    "#     running_loss = 0.0\n",
    "#     valid_running_loss = 0.0\n",
    "#     global_step = 0\n",
    "#     train_loss_list = []\n",
    "#     valid_loss_list = []\n",
    "#     global_steps_list = []\n",
    "\n",
    "#     # training loop\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for (labels, title, text, titletext), _ in train_loader:\n",
    "#             labels = labels.type(torch.LongTensor)           \n",
    "#             labels = labels.to(device)\n",
    "#             titletext = titletext.type(torch.LongTensor)  \n",
    "#             titletext = titletext.to(device)\n",
    "#             output = model(titletext, labels)\n",
    "#             loss, _ = output\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # update running values\n",
    "#             running_loss += loss.item()\n",
    "#             global_step += 1\n",
    "\n",
    "#             # evaluation step\n",
    "#             if global_step % eval_every == 0:\n",
    "#                 model.eval()\n",
    "#                 with torch.no_grad():                    \n",
    "\n",
    "#                     # validation loop\n",
    "#                     for (labels, title, text, titletext), _ in valid_loader:\n",
    "#                         labels = labels.type(torch.LongTensor)           \n",
    "#                         labels = labels.to(device)\n",
    "#                         titletext = titletext.type(torch.LongTensor)  \n",
    "#                         titletext = titletext.to(device)\n",
    "#                         output = model(titletext, labels)\n",
    "#                         loss, _ = output\n",
    "                        \n",
    "#                         valid_running_loss += loss.item()\n",
    "\n",
    "#                 # evaluation\n",
    "#                 average_train_loss = running_loss / eval_every\n",
    "#                 average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "#                 train_loss_list.append(average_train_loss)\n",
    "#                 valid_loss_list.append(average_valid_loss)\n",
    "#                 global_steps_list.append(global_step)\n",
    "\n",
    "#                 # resetting running values\n",
    "#                 running_loss = 0.0                \n",
    "#                 valid_running_loss = 0.0\n",
    "#                 model.train()\n",
    "\n",
    "#                 # print progress\n",
    "#                 print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "#                       .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "#                               average_train_loss, average_valid_loss))\n",
    "                \n",
    "#                 # checkpoint\n",
    "#                 if best_valid_loss > average_valid_loss:\n",
    "#                     best_valid_loss = average_valid_loss\n",
    "#                     save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "#                     save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "#     save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "#     print('Finished Training!')\n",
    "\n",
    "# model = BERT().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# train(model=model, optimizer=optimizer)\n",
    "\n",
    "# train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "# plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "# plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "# plt.xlabel('Global Steps')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show() \n",
    "\n",
    "# def evaluate(model, test_loader):\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for (labels, title, text, titletext), _ in test_loader:\n",
    "\n",
    "#                 labels = labels.type(torch.LongTensor)           \n",
    "#                 labels = labels.to(device)\n",
    "#                 titletext = titletext.type(torch.LongTensor)  \n",
    "#                 titletext = titletext.to(device)\n",
    "#                 output = model(titletext, labels)\n",
    "\n",
    "#                 _, output = output\n",
    "#                 y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "#                 y_true.extend(labels.tolist())\n",
    "    \n",
    "#     print('Classification Report:')\n",
    "#     print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "    \n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "#     ax= plt.subplot()\n",
    "#     sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "#     ax.set_title('Confusion Matrix')\n",
    "\n",
    "#     ax.set_xlabel('Predicted Labels')\n",
    "#     ax.set_ylabel('True Labels')\n",
    "\n",
    "#     ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "#     ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "    \n",
    "# best_model = BERT().to(device)\n",
    "\n",
    "# load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "# evaluate(best_model, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
