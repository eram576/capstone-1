{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skillsfuture dataset\n",
    "sf = pd.read_excel(\"C:/Users/valli/Google Drive/Dell/Semester 6/Capstone/Data/Enhanced construct - 16 sectors.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Design', 'Energy and Power', 'Engineering Services',\n",
       "       'Financial Services', 'Healthcare', 'Intellectual Property',\n",
       "       'Landscape', 'Media', 'Social Service',\n",
       "       'Training and Adult Education', 'Wholesale Trade',\n",
       "       'Workplace Safety and Health', 'Infocomm Technology',\n",
       "       'Sea Transport', 'Accountancy', 'Built Environment'], dtype=object)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf['Sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = sf[sf[\"Sector\"]==\"Media\"]['TSC Description'].head(1).to_string(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = sf[sf[\"Sector\"]==\"Infocomm Technology\"]['TSC Description'].head(1).to_string(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "example3 = sf[sf[\"Sector\"]==\"Training and Adult Education\"]['TSC Description'].head(1).to_string(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4 = sf[sf[\"Sector\"]==\"Engineering Services\"]['TSC Description'].head(1).to_string(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "example5 = sf[sf[\"Sector\"]==\"Healthcare\"]['TSC Description'].head(1).to_string(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Portal import *\n",
    "# pd.set_option('display.width', 400)\n",
    "# pd.set_option('display.max_columns', 10)\n",
    "\n",
    "\n",
    "class CareersFuture(Portal):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def read_data(self):\n",
    "        # Reads data\n",
    "        df = pd.read_csv('mycareerfuture2020-09-15.csv', error_bad_lines=False)\n",
    "        return df\n",
    "\n",
    "    def clean_data(self):\n",
    "        df = self.read_data()\n",
    "        # df = df[df['Job Description'].str.contains('value')]\n",
    "        df['Job Description'] = df['Job Description'].astype(str)\n",
    "        df['Job Description'] = df['Job Description'].apply(lambda x: self.strip_html_tags(x))\n",
    "\n",
    "        df['Industry'] = df['Industry'].astype(str)\n",
    "        df['Industry'] = df['Industry'].apply(lambda x: self.str_to_literal(x))\n",
    "\n",
    "        # df['Skills'] = df['Skills'].astype(str)\n",
    "\n",
    "        df['Job Experience Required (years)'] = df['Job Experience Required (years)'].astype(int)\n",
    "\n",
    "        df['Job Monthly Min Sal'] = df['Job Monthly Min Sal'].astype(int)\n",
    "        df['Job Monthly Max Sal'] = df['Job Monthly Max Sal'].astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def str_to_literal(self, text):\n",
    "        try:\n",
    "            # ast.literal_eval function converts takes in a string and converts it into a dictionary\n",
    "            ls = super().str_to_literal(text)\n",
    "            industry = []\n",
    "            for i in ls:\n",
    "                industry.append(i['category'])\n",
    "            return industry\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "careersfuture = CareersFuture(\"careersfuture\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valli\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:314: UserWarning: \"b' . '\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(careersfuture.clean_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry_unlisted'] = df['Industry'].apply(lambda x: ','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Industry_unlisted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_values = df[df['Industry_unlisted'].str.contains('Building and Construction,F&B')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\valli\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: requests in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (1.7.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (2.2.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\valli\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\valli\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\valli\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_samples = df['Job Description']\n",
    "df_samples = pd.DataFrame(sentence_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['Job Description'][:1000].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_idx = 4\n",
    "print(\"======input sentence======\")\n",
    "print(df['Job Description'][check_idx])\n",
    "breaken_words = []\n",
    "for idx in tokenized[check_idx]:\n",
    "    breaken_words.append(tokenizer.convert_ids_to_tokens(idx))\n",
    "print(\"======after breaking======\")\n",
    "print(\" \".join(breaken_words))\n",
    "print(\"======Convert to IDs======\")\n",
    "print((tokenized[check_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1917)\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101 1996 3026 ...    0    0    0]\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "print(padded[0])\n",
    "print(tokenizer.convert_ids_to_tokens(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape\n",
    "input_ids = torch.tensor(padded).long() \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1917) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-c1fe2bc0da0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m## the output here is a tuple of (last_hidden_state, pooler_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    962\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m         )\n\u001b[0;32m    966\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"absolute\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[0membeddings\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1917) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ## the output here is a tuple of (last_hidden_state, pooler_output)\n",
    "    model_outputs = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last_hidden state\n",
    "last_hidden_state = model_outputs[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9707657\n"
     ]
    }
   ],
   "source": [
    "df_model = df[['Job Description', 'Industry_unlisted']]\n",
    "\n",
    "print(df_model['Job Description'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ['Information Technology', 'Engineering', 'F&B', 'Banking and Finance', 'Accounting / Auditing / Taxation', 'Building and Construction', 'Logistics / Supply Chain', 'Others', 'Education and Training', 'Sales / Retail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_model[df_model['Industry_unlisted'].isin(ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_plot(index):\n",
    "#     example = df_selected[df_selected.index == index][['Job Description', 'Industry_unlisted']].values[0]\n",
    "#     if len(example) > 0:\n",
    "#         print(example[0])\n",
    "#         print('Industry:', example[1])\n",
    "# print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valli\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    text = lemmatize_stemming(text)\n",
    "    return text\n",
    "    \n",
    "df_selected['Job Description'] = df_selected['Job Description'].apply(clean_text)\n",
    "# print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2783533"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected['Job Description'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_selected['Job Description']\n",
    "y = df_selected['Industry_unlisted']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5855299036538811\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "          Information Technology       0.96      0.54      0.69       377\n",
      "                     Engineering       0.92      0.15      0.26       456\n",
      "                             F&B       0.89      0.43      0.58       379\n",
      "             Banking and Finance       0.97      0.46      0.62       264\n",
      "Accounting / Auditing / Taxation       0.81      0.28      0.42       692\n",
      "       Building and Construction       0.90      0.83      0.86       466\n",
      "        Logistics / Supply Chain       0.47      1.00      0.64      1923\n",
      "                          Others       0.84      0.41      0.55       370\n",
      "          Education and Training       0.00      0.00      0.00       280\n",
      "                  Sales / Retail       1.00      0.02      0.03       294\n",
      "\n",
      "                        accuracy                           0.59      5501\n",
      "                       macro avg       0.78      0.41      0.47      5501\n",
      "                    weighted avg       0.70      0.59      0.53      5501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valli\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# nb = Pipeline([('vect', CountVectorizer()),\n",
    "#                ('tfidf', TfidfTransformer()),\n",
    "#                ('clf', MultinomialNB()),\n",
    "#               ])\n",
    "# nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# y_pred = nb.predict(X_test)\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred,target_names=ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.iloc[10605].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Information Technology', 'Banking and Finance'], dtype='<U32')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb.predict(df_selected.iloc[10605])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Information Technology', 'Information Technology'], dtype='<U32')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb.predict(df_selected.iloc[2701])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8180330848936557\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "          Information Technology       0.87      0.92      0.89       377\n",
      "                     Engineering       0.84      0.71      0.77       456\n",
      "                             F&B       0.78      0.84      0.81       379\n",
      "             Banking and Finance       0.84      0.95      0.89       264\n",
      "Accounting / Auditing / Taxation       0.85      0.61      0.71       692\n",
      "       Building and Construction       0.91      0.94      0.93       466\n",
      "        Logistics / Supply Chain       0.80      0.97      0.88      1923\n",
      "                          Others       0.76      0.85      0.80       370\n",
      "          Education and Training       0.88      0.05      0.09       280\n",
      "                  Sales / Retail       0.74      0.70      0.72       294\n",
      "\n",
      "                        accuracy                           0.82      5501\n",
      "                       macro avg       0.83      0.75      0.75      5501\n",
      "                    weighted avg       0.82      0.82      0.80      5501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Banking and Finance', 'Banking and Finance'], dtype='<U32')"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.predict(df_selected.iloc[10605])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valli\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.833484820941647\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "          Information Technology       0.86      0.88      0.87       377\n",
      "                     Engineering       0.84      0.78      0.81       456\n",
      "                             F&B       0.80      0.79      0.80       379\n",
      "             Banking and Finance       0.91      0.95      0.93       264\n",
      "Accounting / Auditing / Taxation       0.80      0.78      0.79       692\n",
      "       Building and Construction       0.94      0.91      0.92       466\n",
      "        Logistics / Supply Chain       0.89      0.91      0.90      1923\n",
      "                          Others       0.81      0.80      0.80       370\n",
      "          Education and Training       0.43      0.46      0.45       280\n",
      "                  Sales / Retail       0.75      0.68      0.71       294\n",
      "\n",
      "                        accuracy                           0.83      5501\n",
      "                       macro avg       0.80      0.80      0.80      5501\n",
      "                    weighted avg       0.83      0.83      0.83      5501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                \n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Banking and Finance', 'Banking and Finance'], dtype=object)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(df_selected.iloc[10605])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_results = pd.DataFrame(X_test).reset_index()\n",
    "Y_test = pd.DataFrame(y_test).reset_index()\n",
    "Y_pred = pd.DataFrame(y_pred).reset_index()\n",
    "log_results = pd.concat([log_results, Y_test['Industry_unlisted'], Y_pred[0]], axis = 1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_results.columns = ['Row', 'Job Description', 'Industry', 'Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_results[log_results['Industry'] != log_results['Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Others'], dtype=object)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict([example1])\n",
    "#Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sales / Retail'], dtype=object)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict([example2])\n",
    "#IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Education and Training'], dtype=object)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict([example3])\n",
    "#Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Engineering'], dtype=object)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict([example4])\n",
    "#Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Information Technology'], dtype=object)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict([example5])\n",
    "#Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\valli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm = df[['Job Description', 'Industry_unlisted']]\n",
    "processed = df_tm['Job Description'].map(preprocess)\n",
    "processed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abl\n",
      "1 accord\n",
      "2 analyz\n",
      "3 annual\n",
      "4 aspect\n",
      "5 assist\n",
      "6 attend\n",
      "7 candid\n",
      "8 clean\n",
      "9 code\n",
      "10 complaint\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3),\n",
       " (19, 1),\n",
       " (45, 1),\n",
       " (53, 1),\n",
       " (105, 4),\n",
       " (111, 4),\n",
       " (129, 1),\n",
       " (134, 1),\n",
       " (136, 1),\n",
       " (138, 4),\n",
       " (144, 3),\n",
       " (157, 1),\n",
       " (167, 1),\n",
       " (196, 1),\n",
       " (198, 6),\n",
       " (206, 1),\n",
       " (208, 2),\n",
       " (216, 4),\n",
       " (229, 1),\n",
       " (254, 2),\n",
       " (266, 2),\n",
       " (270, 1),\n",
       " (284, 1),\n",
       " (287, 2),\n",
       " (288, 2),\n",
       " (290, 1),\n",
       " (293, 1),\n",
       " (295, 1),\n",
       " (297, 2),\n",
       " (311, 4),\n",
       " (315, 1),\n",
       " (326, 1),\n",
       " (331, 1),\n",
       " (335, 2),\n",
       " (347, 1),\n",
       " (352, 2),\n",
       " (362, 1),\n",
       " (373, 1),\n",
       " (376, 2),\n",
       " (400, 1),\n",
       " (411, 1),\n",
       " (412, 1),\n",
       " (419, 1),\n",
       " (425, 2),\n",
       " (433, 1),\n",
       " (441, 2),\n",
       " (445, 4),\n",
       " (449, 1),\n",
       " (455, 1),\n",
       " (498, 2),\n",
       " (499, 2),\n",
       " (516, 3),\n",
       " (553, 1),\n",
       " (573, 1),\n",
       " (578, 1),\n",
       " (592, 2),\n",
       " (606, 2),\n",
       " (657, 2),\n",
       " (658, 1),\n",
       " (743, 1),\n",
       " (755, 4),\n",
       " (774, 1),\n",
       " (800, 2),\n",
       " (828, 1),\n",
       " (863, 2),\n",
       " (887, 1),\n",
       " (904, 1),\n",
       " (910, 3),\n",
       " (974, 1),\n",
       " (1003, 1),\n",
       " (1044, 1),\n",
       " (1070, 7),\n",
       " (1137, 1),\n",
       " (1145, 1),\n",
       " (1184, 1),\n",
       " (1203, 2),\n",
       " (1214, 1),\n",
       " (1215, 3),\n",
       " (1227, 1),\n",
       " (1261, 1),\n",
       " (1315, 1),\n",
       " (1591, 1),\n",
       " (1741, 2),\n",
       " (2022, 1),\n",
       " (2168, 1),\n",
       " (2822, 1),\n",
       " (2908, 1),\n",
       " (2909, 1),\n",
       " (2912, 1),\n",
       " (3248, 7)]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.02836967302244393),\n",
      " (1, 0.056512122877126675),\n",
      " (2, 0.07485220306391939),\n",
      " (3, 0.07976544675713589),\n",
      " (4, 0.08087816759178439),\n",
      " (5, 0.03244767513970287),\n",
      " (6, 0.06645605270427057),\n",
      " (7, 0.03329064886221295),\n",
      " (8, 0.07612410639169918),\n",
      " (9, 0.07250789405860913),\n",
      " (10, 0.20082338925800625),\n",
      " (11, 0.05950843122171499),\n",
      " (12, 0.05072703842597369),\n",
      " (13, 0.06718451253848687),\n",
      " (14, 0.1844223304558999),\n",
      " (15, 0.18247912067851063),\n",
      " (16, 0.049644970510950466),\n",
      " (17, 0.13635244079568923),\n",
      " (18, 0.055763617725171216),\n",
      " (19, 0.0490811259715086),\n",
      " (20, 0.062277519623150174),\n",
      " (21, 0.10079487309888514),\n",
      " (22, 0.07427661004717229),\n",
      " (23, 0.06992195039628246),\n",
      " (24, 0.07638356581507297),\n",
      " (25, 0.08934323997423581),\n",
      " (26, 0.07545194285225845),\n",
      " (27, 0.0673528854829313),\n",
      " (28, 0.06940618187857119),\n",
      " (29, 0.023383341787665032),\n",
      " (30, 0.10772174646367379),\n",
      " (31, 0.07167065287577812),\n",
      " (32, 0.2144541896435618),\n",
      " (33, 0.04170273477403491),\n",
      " (34, 0.094822729251375),\n",
      " (35, 0.047439786718102034),\n",
      " (36, 0.03272955402863433),\n",
      " (37, 0.10954966679565073),\n",
      " (38, 0.04014273518505125),\n",
      " (39, 0.04564520803903264),\n",
      " (40, 0.03796723368411171),\n",
      " (41, 0.0890366560220194),\n",
      " (42, 0.11684106608231337),\n",
      " (43, 0.05525457853166217),\n",
      " (44, 0.2165763133991881),\n",
      " (45, 0.031518034318405906),\n",
      " (46, 0.029476061639903746),\n",
      " (47, 0.09501577458461578),\n",
      " (48, 0.026262312192145764),\n",
      " (49, 0.1359014214698195),\n",
      " (50, 0.02266306695889609),\n",
      " (51, 0.04034124285147325),\n",
      " (52, 0.10006365901831966),\n",
      " (53, 0.02818836365871988),\n",
      " (54, 0.17503697113668487),\n",
      " (55, 0.03273646650288168),\n",
      " (56, 0.4950914784171508),\n",
      " (57, 0.09231684636403323),\n",
      " (58, 0.0502960501817567),\n",
      " (59, 0.12294640886032689),\n",
      " (60, 0.27121439043643064),\n",
      " (61, 0.06121014846723075),\n",
      " (62, 0.07687394863462099),\n",
      " (63, 0.050079971641949564),\n",
      " (64, 0.1679000441547304),\n",
      " (65, 0.04495114811376052),\n",
      " (66, 0.05509947683871107),\n",
      " (67, 0.08299404443975322),\n",
      " (68, 0.07844005030112913),\n",
      " (69, 0.2833889445546944),\n",
      " (70, 0.08446108219993596),\n",
      " (71, 0.12342666236638607),\n",
      " (72, 0.10366803608256636),\n",
      " (73, 0.0654543810742305)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"busi\" + 0.015*\"develop\" + 0.014*\"project\" + 0.011*\"client\" + 0.010*\"product\" + 0.009*\"custom\" + 0.009*\"market\" + 0.008*\"support\" + 0.008*\"strong\" + 0.008*\"servic\"\n",
      "Topic: 1 \n",
      "Words: 0.024*\"secur\" + 0.015*\"oper\" + 0.015*\"support\" + 0.014*\"network\" + 0.012*\"ensur\" + 0.011*\"perform\" + 0.011*\"system\" + 0.009*\"servic\" + 0.009*\"knowledg\" + 0.009*\"provid\"\n",
      "Topic: 2 \n",
      "Words: 0.036*\"data\" + 0.014*\"busi\" + 0.011*\"develop\" + 0.009*\"analyt\" + 0.008*\"product\" + 0.008*\"build\" + 0.008*\"process\" + 0.008*\"technolog\" + 0.007*\"model\" + 0.007*\"learn\"\n",
      "Topic: 3 \n",
      "Words: 0.018*\"product\" + 0.014*\"good\" + 0.013*\"abl\" + 0.012*\"oper\" + 0.012*\"perform\" + 0.011*\"equip\" + 0.011*\"custom\" + 0.011*\"ensur\" + 0.008*\"process\" + 0.008*\"duti\"\n",
      "Topic: 4 \n",
      "Words: 0.011*\"care\" + 0.010*\"provid\" + 0.010*\"research\" + 0.009*\"patient\" + 0.008*\"assist\" + 0.008*\"school\" + 0.008*\"nurs\" + 0.008*\"candid\" + 0.008*\"educ\" + 0.007*\"singapor\"\n",
      "Topic: 5 \n",
      "Words: 0.016*\"account\" + 0.013*\"custom\" + 0.013*\"ensur\" + 0.013*\"prepar\" + 0.011*\"servic\" + 0.010*\"assist\" + 0.010*\"duti\" + 0.010*\"abl\" + 0.009*\"report\" + 0.008*\"handl\"\n",
      "Topic: 6 \n",
      "Words: 0.022*\"project\" + 0.016*\"traineeship\" + 0.014*\"report\" + 0.014*\"graduat\" + 0.013*\"institut\" + 0.011*\"support\" + 0.011*\"traine\" + 0.009*\"assist\" + 0.009*\"process\" + 0.009*\"univers\"\n",
      "Topic: 7 \n",
      "Words: 0.035*\"design\" + 0.028*\"market\" + 0.018*\"develop\" + 0.015*\"product\" + 0.013*\"media\" + 0.013*\"digit\" + 0.011*\"communic\" + 0.010*\"content\" + 0.009*\"social\" + 0.008*\"campaign\"\n",
      "Topic: 8 \n",
      "Words: 0.033*\"develop\" + 0.018*\"applic\" + 0.016*\"technolog\" + 0.015*\"softwar\" + 0.014*\"design\" + 0.012*\"test\" + 0.010*\"knowledg\" + 0.009*\"engin\" + 0.009*\"servic\" + 0.009*\"solut\"\n",
      "Topic: 9 \n",
      "Words: 0.020*\"custom\" + 0.015*\"sale\" + 0.015*\"engin\" + 0.014*\"project\" + 0.013*\"candid\" + 0.009*\"good\" + 0.009*\"servic\" + 0.008*\"client\" + 0.008*\"product\" + 0.008*\"abl\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6524395942687988\t Topic: 0.033*\"develop\" + 0.018*\"applic\" + 0.016*\"technolog\" + 0.015*\"softwar\" + 0.014*\"design\"\n",
      "Score: 0.2287287563085556\t Topic: 0.035*\"design\" + 0.028*\"market\" + 0.018*\"develop\" + 0.015*\"product\" + 0.013*\"media\"\n",
      "Score: 0.10561554878950119\t Topic: 0.022*\"busi\" + 0.015*\"develop\" + 0.014*\"project\" + 0.011*\"client\" + 0.010*\"product\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Designing and implementing chatbots and its optimisation platforms which are used by millions of Shopee customers and internal chatbot operations\\\n",
    "Communicating with product managers and designers and taking responsibility for the development and testing of products\\\n",
    "Understanding the design and product requirements, whilst being familiar with the architecture and implementing solutions to highly complex and flexible features\\\n",
    "Maintaining and improving our high-quality in-house JavaScript libraries and toolsets ( Example: A complete React UI framework following our design guidelines)\\\n",
    "Participating in rigorous and candid code reviews with peers\\\n",
    "Participating in regular internal technology sharings and other regional tech events'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.009*\"project\" + 0.009*\"site\" + 0.008*\"construct\" + 0.006*\"draw\" + 0.006*\"engin\" + 0.006*\"safeti\" + 0.005*\"children\" + 0.005*\"coordin\" + 0.005*\"design\" + 0.005*\"prepar\"\n",
      "Topic: 1 Word: 0.015*\"market\" + 0.009*\"media\" + 0.007*\"content\" + 0.007*\"digit\" + 0.007*\"campaign\" + 0.007*\"social\" + 0.006*\"brand\" + 0.005*\"sale\" + 0.005*\"creativ\" + 0.005*\"design\"\n",
      "Topic: 2 Word: 0.007*\"equip\" + 0.006*\"machin\" + 0.005*\"mainten\" + 0.005*\"product\" + 0.004*\"repair\" + 0.004*\"perform\" + 0.004*\"custom\" + 0.004*\"manufactur\" + 0.004*\"oper\" + 0.004*\"care\"\n",
      "Topic: 3 Word: 0.011*\"food\" + 0.006*\"kitchen\" + 0.005*\"custom\" + 0.005*\"duti\" + 0.005*\"restaur\" + 0.004*\"order\" + 0.004*\"clean\" + 0.004*\"cook\" + 0.004*\"abl\" + 0.004*\"stock\"\n",
      "Topic: 4 Word: 0.006*\"custom\" + 0.005*\"singtel\" + 0.005*\"thank\" + 0.004*\"hair\" + 0.004*\"servic\" + 0.004*\"shipment\" + 0.004*\"product\" + 0.003*\"ship\" + 0.003*\"day\" + 0.003*\"abl\"\n",
      "Topic: 5 Word: 0.006*\"dental\" + 0.004*\"weld\" + 0.003*\"resum\" + 0.003*\"candid\" + 0.003*\"licenc\" + 0.003*\"equip\" + 0.003*\"abl\" + 0.003*\"engin\" + 0.003*\"repair\" + 0.003*\"project\"\n",
      "Topic: 6 Word: 0.010*\"sale\" + 0.006*\"custom\" + 0.005*\"client\" + 0.005*\"account\" + 0.004*\"busi\" + 0.004*\"candid\" + 0.004*\"research\" + 0.003*\"market\" + 0.003*\"servic\" + 0.003*\"administr\"\n",
      "Topic: 7 Word: 0.006*\"secur\" + 0.006*\"data\" + 0.006*\"develop\" + 0.005*\"technolog\" + 0.005*\"test\" + 0.005*\"softwar\" + 0.005*\"design\" + 0.005*\"applic\" + 0.004*\"network\" + 0.004*\"solut\"\n",
      "Topic: 8 Word: 0.015*\"nurs\" + 0.012*\"patient\" + 0.007*\"warehous\" + 0.007*\"care\" + 0.007*\"clinic\" + 0.005*\"medic\" + 0.005*\"load\" + 0.004*\"good\" + 0.004*\"deliveri\" + 0.004*\"licens\"\n",
      "Topic: 9 Word: 0.006*\"traineeship\" + 0.006*\"institut\" + 0.005*\"account\" + 0.005*\"busi\" + 0.005*\"graduat\" + 0.005*\"project\" + 0.005*\"traine\" + 0.005*\"financi\" + 0.004*\"process\" + 0.004*\"develop\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example1:  Create 2D animated sequences for incorporation into animated films, videos, games or other media content\n",
      "Score: 0.9181568026542664\t Topic: 0.035*\"design\" + 0.028*\"market\" + 0.018*\"develop\" + 0.015*\"product\" + 0.013*\"media\"\n",
      "\n",
      "Example2:  Manage, maintain and grow the sales and relationships with a specific customer or set of accounts. This includes in-depth customer engagement, relationship-building and provision of quality solutions and service to address customers' needs efficiently and generate revenue\n",
      "Score: 0.9624866247177124\t Topic: 0.022*\"busi\" + 0.015*\"develop\" + 0.014*\"project\" + 0.011*\"client\" + 0.010*\"product\"\n",
      "\n",
      "Example3:  Design accreditation strucures and award learning qualifications based on assessments of alignment with accreditation requirements\n",
      "Score: 0.756336510181427\t Topic: 0.022*\"project\" + 0.016*\"traineeship\" + 0.014*\"report\" + 0.014*\"graduat\" + 0.013*\"institut\"\n",
      "\n",
      "Example4:  Generate 3D models using a variety of modelling software to represent characteristics of a real-world system\n",
      "Score: 0.9099632501602173\t Topic: 0.036*\"data\" + 0.014*\"busi\" + 0.011*\"develop\" + 0.009*\"analyt\" + 0.008*\"product\"\n",
      "\n",
      "Example5:  Provide support to team in the provision of ambulatory care services in pharmacist-led services\n",
      "Score: 0.8874416947364807\t Topic: 0.011*\"care\" + 0.010*\"provid\" + 0.010*\"research\" + 0.009*\"patient\" + 0.008*\"assist\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "for i in [example1, example2, example3, example4, example5]:\n",
    "    print(\"Example\" + str(j)+ \": \" + i)\n",
    "    j+=1\n",
    "    bow_vector = dictionary.doc2bow(preprocess(i))\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "        print(\"\")\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stuff that isn't happening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"to be or not to be\"\n",
    "# tokens = nltk.word_tokenize(text)\n",
    "# bigrm = nltk.bigrams(tokens)\n",
    "# print(*map(' '.join, bigrm), sep=', ')\n",
    "\n",
    "jobD = [row.split() for row in text]\n",
    "phrases = Phrases(jobD, min_count=10, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[jobD]\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "bigram=list(ngrams(df_bigram['Job Description'],2))\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"Hi How are you? i am fine and you\"\n",
    "\n",
    "n_grams = ngrams(df_bigram['Job Description'], 2)\n",
    "\n",
    "for grams in n_grams :\n",
    "    print(grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "df_w2v = df[['Job Description', 'Industry_unlisted']]\n",
    "train, test = train_test_split(df_w2v, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['Job Description']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['Job Description']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['Industry_unlisted'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.Industry_unlisted))\n",
    "print(classification_report(test.Industry_unlisted, y_pred,target_names=ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\valli\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.14.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\valli\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\valli\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\valli\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(df) * .7)\n",
    "train_posts = df_w2v['Job Description'][:train_size]\n",
    "train_tags = df_w2v['Industry_unlisted'][:train_size]\n",
    "\n",
    "test_posts = df_w2v['Job Description'][train_size:]\n",
    "test_tags = df_w2v['Industry_unlisted'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
