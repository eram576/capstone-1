{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import texthero as hero\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFW_16  = pd.read_csv('Enhanced construct - 16 sectors.xlsx - Job Role_TSC.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFW_13  = pd.read_csv('Enhanced construct - 13 sectors.xlsx - Job Role_TSC.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SFW_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SFW_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfw = pd.concat([SFW_16, SFW_13], sort = False, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = np.load('replace_spelling.npy', allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_keys = mydict.item().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_values = mydict.item().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_dict = dict(zip(spelling_keys, spelling_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all(text, mydict):\n",
    "    for gb, us in mydict.items():\n",
    "        text = text.replace(us, gb)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_col(sfw, spelling_dict):\n",
    "    \n",
    "    sfw['prof + knowlg + ability'] = \"\"\n",
    "    for i in range(0, len(sfw)):\n",
    "        sfw.loc[i, 'prof + knowlg + ability'] = str(sfw.loc[i, 'Proficiency']) + str(sfw.loc[i, 'Knowledge']) + str(sfw.loc[i, 'Abilities'])\n",
    "    \n",
    "    sfw['clean_docs'] = sfw['prof + knowlg + ability'].apply(lambda x: x.replace('\\n', '. '))\n",
    "    sfw['clean_docs'] = sfw['clean_docs'].apply(lambda x: x.replace('•', ' '))\n",
    "    sfw['clean_docs'] = sfw['prof + knowlg + ability'].apply(lambda x: x.replace('\\n', '. '))\n",
    "    sfw['clean_docs'] = sfw['clean_docs'].apply(lambda x: x.replace('•', ' '))   \n",
    "    sfw['clean_docs'] = sfw['clean_docs'].apply(lambda x: replace_all(x, spelling_dict))\n",
    "    sfw['clean_docs'] = sfw['clean_docs'].astype(str).apply(clean_text)\n",
    "    sfw['Proficiency Level'] = sfw['Proficiency Level'].str.split(n=1).str[1]\n",
    "    \n",
    "    return sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfw = new_col(sfw, spelling_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfw = sfw[[\"Job Role\",\"clean_docs\", \"Sector\", \"Skill Title\", \"Proficiency Level\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_values(df):\n",
    "    jd_list = []\n",
    "    counter = 0\n",
    "    for row in df[1]:\n",
    "        jd=\"\"\n",
    "        counter+=1\n",
    "        for val in row:\n",
    "            jd+=val[0]\n",
    "        jd_list.append(jd)\n",
    "    return jd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skill_description(sfw):\n",
    "    sfw_concatenated = sfw.groupby(['Sector','Skill Title','Proficiency Level'])['clean_docs'].unique()\n",
    "    sfw_concatenated = pd.DataFrame(sfw_concatenated)\n",
    "    sfw_concatenated  = sfw_concatenated.groupby(['Sector','Skill Title'])['clean_docs']\n",
    "    sfw_concatenated = pd.DataFrame(sfw_concatenated)\n",
    "    \n",
    "    jd_list = concat_values(sfw_concatenated)\n",
    "    \n",
    "    sfw_concatenated['Skill description'] = jd_list\n",
    "    sfw_concatenated[['Sector', 'Skill Title']] = pd.DataFrame(sfw_concatenated[0].tolist()) \n",
    "    sfw_concatenated = sfw_concatenated.drop(columns = [0, 1])\n",
    "    sfw_concatenated = sfw_concatenated[['Sector', 'Skill Title', 'Skill description']]\n",
    "    \n",
    "    sfw_new = sfw.merge(sfw_concatenated, on=[\"Sector\",\"Skill Title\"], how = 'left')\n",
    "\n",
    "    sf_model_final = sfw_new[['Job Role', 'Sector', 'Skill Title', 'Skill description']]\n",
    "    \n",
    "    sf_model_final['Skill description'] = hero.remove_stopwords(sf_model_final['Skill description'].astype(str))\n",
    "    sf_model_final['Skill description'] = hero.remove_urls(sf_model_final['Skill description'])\n",
    "    sf_model_final['Skill description'] = hero.tokenize(sf_model_final['Skill description']).astype(str)\n",
    "    sf_model_final['Skill description'] = hero.clean(sf_model_final['Skill description'])\n",
    "\n",
    "    return sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final = create_skill_description(sfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final[sf_model_final['Job Role'].str.contains('Head')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge skills of each job role into a list of skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def clear_hierarchy(title):\n",
    "    \n",
    "    patterns = [\"Senior\", \"Junior\", \"Manager\", \"Executive\", \"Director\", \"Lead\", \"Head of\", \"(Specialist)\", \"Specialist\"]\n",
    "    \n",
    "    for s in patterns:\n",
    "        title = re.sub(s, '', title)\n",
    "        \n",
    "    split = title.split()\n",
    "    if (len(split)!=0):\n",
    "        \n",
    "        if (split[0] == 'Assistant') or (split[0]=='Head'):\n",
    "            title = \" \".join(split[1:])\n",
    "\n",
    "        if title[-2:]==\"or\":\n",
    "            title = title[:-2].strip()+\"ing\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    list_ = []\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        list_.append(lemmatize_stemming(w))\n",
    "    \n",
    "    title = ' '.join(list_)\n",
    "  \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final['Job_Role_Normal'] = sf_model_final['Job Role'].apply(lambda x: clear_hierarchy(x))\n",
    "sf_model_final['Job_Role_Stemmed'] = sf_model_final.Job_Role_Normal.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = sf_model_final[sf_model_final['Job_Role_Normal']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final = sf_model_final.groupby(['Job_Role_Stemmed', 'Sector']).agg({'Job Role': lambda series: list(series), 'Skill description': lambda series: list(series), 'Skill Title':lambda series: list(series)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Job Role'] = sf_model_final['Job Role'].apply(lambda x: list(dict.fromkeys(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(x):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        y.append(x[i].lower())\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Skill Title'] = sf_model_final['Skill Title'].apply(lambda x: to_lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final['Skill Title'] = sf_model_final['Skill Title'].astype(str)\n",
    "sf_model_final['Skill Title'] = sf_model_final['Skill Title'].apply(lambda x: x.replace(\"'\", \"\"))\n",
    "sf_model_final['Skills_unlisted'] = sf_model_final['Skill Title'].apply(lambda x: x.replace('[','').replace(']','').replace(\",\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_model_final.to_csv('sf_model_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
