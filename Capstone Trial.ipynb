{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install texthero\n",
    "# to convert MSword doc to txt for processing.\n",
    "!pip install docx2txt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "import texthero as hero\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "import docx2txt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import ngrams\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords# Import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fsspec==0.7.4 \n",
    "basefn = \"C://Users//valli//Google Drive//Dell//Semester 6//Capstone//Data//\"\n",
    "jobstreet = pd.read_csv(basefn + \"jobstreet2020-09-28.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string literal of dictionary into dictionary\n",
    "def str_to_dic(x):\n",
    "    try:\n",
    "        new = ast.literal_eval(x)['label']\n",
    "        return new\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobstreet['Job Description'] = jobstreet['Job Description'].apply(lambda x: striphtml(str(x)))\n",
    "jobstreet = jobstreet.dropna(subset=['Job Description'])\n",
    "\n",
    "jobstreet['Industry'] = jobstreet['Industry'].apply(lambda x: str_to_dic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobstreet['Job Description'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobstreet.iloc[:, [1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['Job Description'] = hero.remove_stopwords(jobs['Job Description'])\n",
    "jobs['Job Description'] = hero.remove_urls(jobs['Job Description'])\n",
    "jobs['Job Description'] = hero.tokenize(jobs['Job Description']).astype(str)\n",
    "jobs['Job Description'] = hero.clean(jobs['Job Description'])\n",
    "# stemmer = SnowballStemmer('english')\n",
    "# jobs['stemmed jd'] = jobs['Job Description'].apply(lambda x: [stemmer.stem(y) for y in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['Job Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = hero.visualization.top_words(jobs.loc[:, 'Job Description']).head(20)\n",
    "top_20.plot.bar(rot=90, title=\"Top 20 words in corpus\");\n",
    "plt.show(block=True)\n",
    "#to be added to the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero.wordcloud(jobs.loc[:, 'Job Description'], max_words=100)\n",
    "#to be added to the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.loc[jobs['Job Description'].str.contains('money', na=False)].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchfor = ['enron stock', 'sell stock', 'stock bonus', 'sell enron stock']\n",
    "\n",
    "# filtered_emails  = df_corpus.loc[df_corpus['content'].str.contains('|'.join(searchfor), na=False)]\n",
    "# filtered_emails.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobs.drop_duplicates('Job Description')\n",
    "jobs = jobs.dropna(subset=['Industry', 'Job Title'])  # take out job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = jobs['Industry'].value_counts().index.tolist()\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "sns.countplot(x = jobs['Industry'], order = industries, ax = ax)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "industries = jobs['Industry'].value_counts().index.tolist()\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "sns.countplot(x = jobs['Industry'], order = industries[1:], ax = ax)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs['Industry'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['Industry'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_revised = jobs[jobs[\"Industry\"] != \"Human Resources Management / Consulting\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising, clustering, heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kitakoj18/exploring-wine-descriptions-with-nlp-and-kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(jobs_revised[\"Job Description\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = vectorizer.get_feature_names()\n",
    "word_features[100:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 10, n_init = 5, n_jobs = -1)\n",
    "kmeans.fit(X)\n",
    "\n",
    "#stem and lemmatise\n",
    "# topic modelling vs clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(word_features[word] for word in centroid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_revised['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = jobs_revised.groupby(['cluster', 'Industry']).size()\n",
    "fig2, ax2 = plt.subplots(figsize = (30, 15))\n",
    "sns.heatmap(clusters.unstack(level = 'Industry'), ax = ax2, cmap = 'Reds')\n",
    "\n",
    "ax2.set_xlabel('Industry', fontdict = {'weight': 'bold', 'size': 24})\n",
    "ax2.set_ylabel('cluster', fontdict = {'weight': 'bold', 'size': 24})\n",
    "for label in ax2.get_xticklabels():\n",
    "    label.set_size(16)\n",
    "    label.set_weight(\"bold\")\n",
    "for label in ax2.get_yticklabels():\n",
    "    label.set_size(16)\n",
    "    label.set_weight(\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_selected = jobs_revised[jobs_revised[\"Industry\"].isin([\"Consulting (IT, Science, Engineering & Technical)\", 'Non-Profit Organisation / Social Services / NGO', 'Healthcare / Medical',  'Computer / Information Technology (Software)', 'Banking / Financial Services', 'BioTechnology/Pharmaceutical/Clinical research', 'Computer/Information Technology (Hardware)'  ]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X2 = vectorizer.fit_transform(jobs_selected[\"Job Description\"].values)\n",
    "word_features = vectorizer.get_feature_names()\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4, n_jobs = -1)\n",
    "kmeans.fit(X2)\n",
    "\n",
    "# plt.scatter(X2[:, 0], X2[:, 1], c=prediction, s=50, cmap='viridis')\n",
    "# centers = k.means.cluster_centers_\n",
    "# plt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6)\n",
    "\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(word_features[word] for word in centroid))\n",
    "    \n",
    "jobs_selected['cluster'] = kmeans.labels_\n",
    "\n",
    "clusters = jobs_selected.groupby(['cluster', 'Industry']).size()\n",
    "fig2, ax2 = plt.subplots(figsize = (30, 15))\n",
    "sns.heatmap(clusters.unstack(level = 'Industry'), ax = ax2, cmap = 'Reds')\n",
    "\n",
    "ax2.set_xlabel('Industry', fontdict = {'weight': 'bold', 'size': 24})\n",
    "ax2.set_ylabel('cluster', fontdict = {'weight': 'bold', 'size': 24})\n",
    "for label in ax2.get_xticklabels():\n",
    "    label.set_size(16)\n",
    "    label.set_weight(\"bold\")\n",
    "for label in ax2.get_yticklabels():\n",
    "    label.set_size(16)\n",
    "    label.set_weight(\"bold\")\n",
    "    \n",
    "# Number of rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/k-means-clustering-8e1e64c1561c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_norm = normalize(X)\n",
    "tf_idf_array = tf_idf_norm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pca = PCA(n_components = 2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(tf_idf_array)\n",
    "kmeans = KMeans(n_clusters=4, max_iter=600, algorithm = 'auto')\n",
    "fitted = kmeans.fit(Y_sklearn)\n",
    "prediction = kmeans.predict(Y_sklearn)\n",
    "plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=prediction, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(word_features[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to work on extracting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = range(1, 10)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters]\n",
    "kmeans\n",
    "\n",
    "score = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))]\n",
    "score\n",
    "\n",
    "plt.plot(number_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/notebooks/Downloads/NLP/11_Word2Vec_via%20Gensim.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobD = [row.split() for row in jobs['Job Description']]\n",
    "phrases = Phrases(jobD, min_count=10, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[jobD]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=10,\n",
    "                     window=3,\n",
    "                     size=55,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.05, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=40, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"health\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"finance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('skills_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['bio', 'medicine', 'computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"software\", \"hardware\"], negative=[\"number\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = []\n",
    "vecs = []\n",
    "for word in w2v_model.wv.vocab:\n",
    "    vocabs.append(word)\n",
    "    vecs.append(w2v_model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vecs = vecs[:100]\n",
    "sub_vocab = vocabs[:100]\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2000, random_state=23)\n",
    "new_values = tsne_model.fit_transform(sub_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(sub_vocab[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Matching - Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gojek = \"Perform analysis, dive deep into data sets, and build reports/dashboards as needed in order to monitor business health and solve business problems. \\\n",
    "Be a driver of action to identify valuable business insights to further scale our business in Indonesia and operations in other countries. \\\n",
    "Evaluate the many iterative experiments we run regularly and their impact to our North Star Metric. \\\n",
    "Be a trusted business advisor, offering guidance in how to most effectively utilize data to solve issues (not just simply responding to requests). \\\n",
    "Engage in hypothesis-driven problem solving, and build processes tools that'll enable your teammates to easily do the same. \\\n",
    "Suggest A/B tests and experimentation for product features; analyze how the key metrics respond and evaluate; guide decisions on whether to increase exposure or stop the experiment. \\\n",
    "Engage and partner with various Product & Engineering teams to conduct research in efforts to improve our business. \\\n",
    "Currently in your third or final year of studies, majoring in either computer science, software engineering, and other technical fields. \\\n",
    "Basic knowledge and experience working with SQL. \\\n",
    "Deep curiosity, able to learn independently, and work well in a team. \\\n",
    "Strong written and verbal English communication abilities. \\\n",
    "Prior experience working on analytical projects. \\\n",
    "Able to write in Python/R and visualize analysis using Tableau/Python. \\\n",
    "Basic knowledge in statistics. \\\n",
    "A keen eye for patterns in data.\"\n",
    "\n",
    "#Summarize the text with ratio 0.1 (10% of the total words.)\n",
    "# text_resume = summarize(text_resume, ratio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopee = \"This opportunity is only for students who are available for a minimum of 12 weeks in the period of May - August 2021. \\\n",
    "You can now apply for our 2021 Data Analytics & Data Science Summer Internships , running from May to August. \\\n",
    "Our internships are designed to provide real-world experience that enables you to put everything you've learned into action and give you an opportunity to join us in driving South East Asia forward, together. \\\n",
    "We strive to make a difference and disrupt the world of transport and financial services.\\\n",
    "Full-time student majoring in Computer Sciences, Engineering, Statistics, Data Science or related fields seeking a matriculated internship\\\n",
    "You are enrolled in a Singapore university, or Singaporean/PR studying abroad\\\n",
    "Previous internship experience in Data Analytics, Computer Science or other related field is an advantage\\\n",
    "Be able to commit full-time from May toAugust 2021, for a minimum of 12 weeks \\\n",
    "Agile and able to work in a fast-paced environment\\\n",
    "Excellent communication, presentation and project management skills \\\n",
    "We require our interns in Singapore to be covered under the Employment Act (EA) and Work Injury Compensation (WICA) act. As such, our internship opportunities are limited to students of any nationality studying in Singapore, and Singaporean/PR studying in any location. \\\n",
    "For more information, view the Ministry of Manpower (MOM) website. If you do not qualify for our internship opportunities, we encourage you to review our Traineeship openings instead!\\\n",
    "Ready to apply? Please submit a resume and cover letter by 31st January 2021 indicating your area of interest from the above mentioned opportunities. We look forward to speaking to you soon!\\\n",
    "Get to know Grab: \\\n",
    "Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We work with governments, drivers, passengers, merchants, and the community, to solve critical problems in Southeast Asia. \\\n",
    "Grab began as a taxi-hailing app in 2012, but we have since extended our product platform to include GrabCar, GrabShare, GrabBike, GrabHitch, GrabExpress, GrabFood, GrabCoach, GrabShuttle, GrabCycle. We recently launched our fintech platform â€“ GrabFinancial, which consists of payments, lending and insurance. Our latest addition is GrabVentures, an in-house incubation platform. We are focused on pioneering new commuting and payment alternatives for drivers and passengers with an emphasis on convenience, safety, and reliability. Currently, we offer services in 8 countries. Our R&D offices are in Singapore, Seattle, Beijing, Bangalore, Jakarta and Vietnam. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride. \\\n",
    "If you share our vision of driving South East Asia forward, apply to join our team today.\"\n",
    "# text = summarize(text, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [gojek, shopee]\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# get the match percentage\n",
    "matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
    "matchPercentage = round(matchPercentage, 2) # round to two decimal\n",
    "print(\"Your resume matches about \" + str(matchPercentage)+ \"% of the job description.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needa try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram=list(ngrams(Sentences.lower().split(),2))\n",
    "trigram=list(ngrams(Sentences.lower().split(),3))\n",
    "fourgram=list(ngrams(Sentences.lower().split(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unneccessary stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit= False\n",
    "for row in range(len(df_corpus)):\n",
    "    if ):\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(10, 47):\n",
    "            if df_corpus.iloc[row, i][0] == \"{\":\n",
    "                df_corpus.iloc[row, 2] = df_corpus.iloc[row, i][0]\n",
    "                #df_corpus.iloc[row, 1] = [' '.join(row.astype(str)) for row in df_corpus[df_corpus.columns[1:i-1]].values]\n",
    "                break\n",
    "            break\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    \"\"\" K Means Clustering\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        k: int , number of clusters\n",
    "        \n",
    "        seed: int, will be randomly set if None\n",
    "        \n",
    "        max_iter: int, number of iterations to run algorithm, default: 200\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "       centroids: array, k, number_features\n",
    "       \n",
    "       cluster_labels: label for each data point\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k, seed = None, max_iter = 200):\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "\n",
    "\n",
    "    def initialise_centroids(self, data):\n",
    "        \"\"\"Randomly Initialise Centroids\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        centroids: array of k centroids chosen as random data points \n",
    "        \"\"\"\n",
    "\n",
    "        initial_centroids = np.random.permutation(data.shape[0])[:self.k]\n",
    "        self.centroids = data[initial_centroids]\n",
    "\n",
    "        return self.centroids\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict which cluster data point belongs to\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        cluster_labels: index which minmises the distance of data to each\n",
    "        cluster\n",
    "        \"\"\"\n",
    "    \n",
    "        return self.assign_clusters(data)\n",
    "\n",
    "    def assign_clusters(self, data):\n",
    "        \"\"\"Compute distance of data from clusters and assign data point\n",
    "           to closest cluster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        cluster_labels: index which minmises the distance of data to each\n",
    "        cluster\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "\n",
    "        dist_to_centroid =  pairwise_distances(data, self.centroids, metric = 'euclidean')\n",
    "        self.cluster_labels = np.argmin(dist_to_centroid, axis = 1)\n",
    "\n",
    "        return  self.cluster_labels\n",
    "\n",
    "\n",
    "    def update_centroids(self, data):\n",
    "        \"\"\"Computes average of all data points in cluster and\n",
    "           assigns new centroids as average of data points\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        centroids: array, k, number_features\n",
    "        \"\"\"\n",
    "\n",
    "        self.centroids = np.array([data[self.cluster_labels == i].mean(axis = 0) for i in range(self.k)])\n",
    "\n",
    "        return self.centroids\n",
    "    \n",
    "\n",
    "    def fit_kmeans(self, data):\n",
    "        \"\"\"\n",
    "        This function contains the main loop to fit the algorithm\n",
    "        Implements initialise centroids and update_centroids\n",
    "        according to max_iter\n",
    "        -----------------------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        instance of kmeans class\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.centroids = self.initialise_centroids(data)\n",
    "\n",
    "        # Main kmeans loop\n",
    "        for iter in range(self.max_iter):\n",
    "\n",
    "            self.cluster_labels = self.assign_clusters(data)\n",
    "            self.centroids = self.update_centroids(data)          \n",
    "            if iter % 100 == 0:\n",
    "                print(\"Running Model Iteration %d \" %iter)\n",
    "        print(\"Model finished running\")\n",
    "        return self    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
